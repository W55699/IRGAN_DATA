{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "\n",
    "from model import Generator, Discriminator\n",
    "from config import irgan_config\n",
    "from data_utils_1 import RecDataset, DataProvider\n",
    "from evaluation.rec_evaluator import RecEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = irgan_config.epochs\n",
    "batch_size = irgan_config.batch_size\n",
    "epochs_d = irgan_config.epochs_d\n",
    "epochs_g = irgan_config.epochs_g\n",
    "emb_dim = irgan_config.emb_dim\n",
    "eta_G = irgan_config.eta_G\n",
    "eta_D = irgan_config.eta_D\n",
    "device = irgan_config.device\n",
    "weight_decay_g = irgan_config.weight_decay_g\n",
    "weight_decay_d = irgan_config.weight_decay_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters and datset-specific parameters\n",
    "rec_dataset = RecDataset(\"./data/ml-1m/\")\n",
    "all_users = rec_dataset.get_users()\n",
    "all_items = rec_dataset.get_items()\n",
    "num_users = rec_dataset.get_num_users()\n",
    "num_items = rec_dataset.get_num_items()\n",
    "bought_mask = rec_dataset.get_bought_mask().to(device)\n",
    "eval_dict = rec_dataset.get_interaction_records(\"test\")\n",
    "train_ui = rec_dataset.get_user_item_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator(num_users, num_items, emb_dim, bought_mask)\n",
    "D = Discriminator(num_users, num_items, emb_dim)\n",
    "G = G.to(device)\n",
    "D = D.to(device)\n",
    "\n",
    "loss_D = nn.BCELoss()\n",
    "optimizer_G = torch.optim.SGD(G.parameters(), momentum = 0.9, lr = eta_G)\n",
    "optimizer_D = torch.optim.SGD(D.parameters(), momentum = 0.9, lr = eta_D, weight_decay = weight_decay_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/10]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-8b061af2323c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mtrain_set_d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_data_for_discriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_ui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mloss_d_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0musers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_set_d\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m             \u001b[0mdis_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0musers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mloss_d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdis_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pzsx0\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 624\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_profile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    625\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pzsx0\\Anaconda3\\lib\\site-packages\\torch\\autograd\\profiler.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, args)\u001b[0m\n\u001b[0;32m    483\u001b[0m         \u001b[1;31m# Stores underlying RecordFunction as a tensor. TODO: move to custom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m         \u001b[1;31m# class (https://github.com/pytorch/pytorch/issues/35026).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 485\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    print(\n",
    "            \"[Epoch %d/%d]\"\n",
    "            % (epoch+1, 10)\n",
    "        )  \n",
    "    time_epoch_start = time.time()\n",
    "    \n",
    "    # ---------------------\n",
    "    #  Train Discriminator\n",
    "    # ---------------------   \n",
    "    D.train()\n",
    "    G.eval()\n",
    "    for epoch_d in range(epochs_d):\n",
    "        time_d_start = time.time()\n",
    "        if(epoch_d % 5 == 0):\n",
    "            train_set_d = dp.prepare_data_for_discriminator(G, train_ui, batch_size = batch_size)\n",
    "        loss_d_epoch = 0\n",
    "        for users, items, labels in train_set_d:\n",
    "            dis_score = D(users,items)\n",
    "            loss_d = loss_D(dis_score, labels)\n",
    "            loss_d_epoch += loss_d.item()\n",
    "            optimizer_D.zero_grad()\n",
    "            loss_d.backward()\n",
    "            optimizer_D.step()\n",
    "            \n",
    "        time_d_end = time.time()\n",
    "        loss_d_epoch /= len(train_set_d)\n",
    "        print(\n",
    "            \"\\t[Discriminator][Epochs %d/%d] [D epoch loss: %6.5f] [Time:%6.5f] \"\n",
    "            % (epoch_d+1, epochs_d, loss_d_epoch, time_d_end - time_d_start)\n",
    "        )\n",
    "       \n",
    "        with torch.no_grad():\n",
    "\n",
    "            torch.save(D.state_dict(),\"./pretrained_models/ml-1m/pretrained_model_discriminator.pkl\")\n",
    "        \n",
    "    # --------------------- \n",
    "    #  Train Generator\n",
    "    #\n",
    "    # For generator\n",
    "    #   Generate K user-item pairs\n",
    "    #   Leveraging Policy Gradient to update parameters of generator\n",
    "    #\n",
    "    # --------------------- \n",
    "    D.eval()\n",
    "    G.train()\n",
    "    for epoch_g in range(epochs_g):\n",
    "        time_g_start = time.time()\n",
    "        train_set_g = dp.prepare_data_for_generator(all_users, batch_size)\n",
    "        loss_g_epoch = 0\n",
    "        for fake_users, in train_set_g:\n",
    "            fake_items, fake_probs, fake_p_n = G.sample_items_for_users(fake_users, k = 256,\\\n",
    "                                                                        temperature=1, lambda_bought=0.2)\n",
    "            fake_users = fake_users.view(-1,1).expand_as(fake_items).contiguous()\n",
    "            fake_users = fake_users.view(-1)\n",
    "            fake_items = fake_items.view(-1)\n",
    "            fake_probs = fake_probs.view(-1)\n",
    "            fake_p_n = fake_p_n.view(-1)\n",
    "            \n",
    "            log_fake_probs = torch.log(fake_probs.clamp(1e-8))\n",
    "            fake_probs = fake_probs.detach()\n",
    "            fake_p_n = fake_p_n.detach()\n",
    "            \n",
    "            reward = (2 * D(fake_users, fake_items) - 1).detach()*(fake_probs/fake_p_n)\n",
    "            loss_g = -torch.mean(log_fake_probs*reward)\n",
    "            loss_g_epoch += loss_g\n",
    "            \n",
    "            optimizer_G.zero_grad()  \n",
    "            loss_g.backward() \n",
    "            optimizer_G.step()\n",
    "        time_g_end = time.time()\n",
    "        print(\n",
    "            \"\\t[Generator][Epochs %d/%d] [G epoch loss: %6.5f] [Time:%6.5f]\"\n",
    "            % (epoch_g + 1, epochs_g, loss_g_epoch, time_g_end - time_g_start )\n",
    "        ) \n",
    "     \n",
    "        with torch.no_grad():\n",
    "\n",
    "            torch.save(G.state_dict(),\"./pretrained_models/ml-1m/pretrained_model_generator.pkl\")\n",
    "    time_epoch_end = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
